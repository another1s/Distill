# Model Distillation
An easy deployed distillation scripts 
## Prerequisities
* textbrewer 
* pytorch
## Details
1. Base model 
    * Roberta
    * Electra
    * Bert
2. Distillation strategy
    * teacher model output + intermediate layer + student output loss
3. Data Augmentation
    * 
## 