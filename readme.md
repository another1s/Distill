# Model Distillation
An easy deployed distillation scripts 
## Prerequisities
* textbrewer 
* pytorch
## Details
1. Base model 
    * Roberta
    * Electra
    * Bert
2. Distillation strategy
    * single teacher
        * teacher model output + MSE(intermediate layers) + student output loss
    * mul-ti teacher
        * 
3. Data Augmentation
    * 
## 